{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM8/JkcAZgO6WwBLhegK6Uc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElaheTorabi/Masters-Thesis/blob/main/Fuel_Data_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fuel Data Preprocessing and Cleaning**"
      ],
      "metadata": {
        "id": "YzdeaTeS16Pz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing necassary packages"
      ],
      "metadata": {
        "id": "v_ZHD7hCsAXJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZqycu4XrMqE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the dataset"
      ],
      "metadata": {
        "id": "mWE6s32NsZG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('....xlsx')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "claaK21dsFF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation"
      ],
      "metadata": {
        "id": "9-gFB4ASs1Rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert date column to datetime\n",
        "df['Date_Hour_Desc'] = pd.to_datetime(df['Date_Hour_Desc'], errors='coerce')\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values('Date_Hour_Desc')\n",
        "\n",
        "# Set timestamp as index for easier plotting\n",
        "df = df.set_index('Date_Hour_Desc')"
      ],
      "metadata": {
        "id": "oeirGk68sz-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find rows where LevelLiters is greater than TotalLiters\n",
        "invalid_rows = df[df['LevelLiters'] > df['TotalLiters']]\n",
        "\n",
        "# Print summary\n",
        "print(f\"Number of records where LevelLiters > TotalLiters: {len(invalid_rows)}\")\n",
        "\n",
        "# Display sample\n",
        "if len(invalid_rows) > 0:\n",
        "    print(\"\\nSample invalid readings:\")\n",
        "    print(invalid_rows[['DEVICE_ID', 'LevelLiters', 'TotalLiters']].head(10))\n",
        "\n",
        "# Save invalid rows separately\n",
        "invalid_rows.to_excel(\"invalid_LevelLiters_vs_TotalLiters.xlsx\", index=True)\n"
      ],
      "metadata": {
        "id": "V-0K4LYCs91p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering Data"
      ],
      "metadata": {
        "id": "KgJCZ_Y_tPST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df[df['LevelLiters'] <= df['TotalLiters']].copy()\n",
        "\n",
        "print(\"Original dataset size:\", len(df))\n",
        "print(\"Cleaned dataset size :\", len(df_clean))\n",
        "print(\"Rows removed:\", len(df) - len(df_clean))\n"
      ],
      "metadata": {
        "id": "Ri9qXQzutFZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bring the datetime index back as a column\n",
        "df_clean = df_clean.reset_index()\n",
        "\n",
        "# Double-check\n",
        "print(df_clean.columns[:5])\n"
      ],
      "metadata": {
        "id": "YlxBzHErtq2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetching hourly temperature data"
      ],
      "metadata": {
        "id": "2aLs-W-ItydW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Ensure datetime\n",
        "df_clean['Date_Hour_Desc'] = pd.to_datetime(df_clean['Date_Hour_Desc'])\n",
        "df_clean = df_clean.sort_values('Date_Hour_Desc')\n",
        "\n",
        "# Unique devices with coordinates\n",
        "device_location_map = df_clean[['DEVICE_ID', 'OGI_LAT', 'OGI_LONG']].drop_duplicates()\n",
        "print(f\"Found {len(device_location_map)} unique devices with coordinates.\")\n",
        "\n",
        "# Determine overall time range\n",
        "start_date = df_clean['Date_Hour_Desc'].min().strftime(\"%Y-%m-%d\")\n",
        "end_date = df_clean['Date_Hour_Desc'].max().strftime(\"%Y-%m-%d\")\n",
        "print(f\"Fetching hourly weather between {start_date} and {end_date}\")\n",
        "\n",
        "weather_records = []\n",
        "\n",
        "# Loop per device\n",
        "for _, row in tqdm(device_location_map.iterrows(), total=len(device_location_map)):\n",
        "    device_id = row['DEVICE_ID']\n",
        "    lat, lon = row['OGI_LAT'], row['OGI_LONG']\n",
        "\n",
        "    try:\n",
        "        url = (\n",
        "            f\"https://archive-api.open-meteo.com/v1/archive?\"\n",
        "            f\"latitude={lat}&longitude={lon}\"\n",
        "            f\"&start_date={start_date}&end_date={end_date}\"\n",
        "            f\"&hourly=temperature_2m&timezone=auto\"\n",
        "        )\n",
        "        r = requests.get(url)\n",
        "        d = r.json()\n",
        "        if \"hourly\" in d and \"time\" in d[\"hourly\"]:\n",
        "            temp_df = pd.DataFrame({\n",
        "                \"datetime\": pd.to_datetime(d[\"hourly\"][\"time\"]),\n",
        "                \"temperature\": d[\"hourly\"][\"temperature_2m\"],\n",
        "                \"DEVICE_ID\": device_id\n",
        "            })\n",
        "            weather_records.append(temp_df)\n",
        "    except Exception as e:\n",
        "        print(f\" Failed for DEVICE_ID {device_id}: {e}\")\n",
        "\n",
        "# Combine all devices’ temperature data\n",
        "df_weather_hourly = pd.concat(weather_records, ignore_index=True)\n",
        "df_weather_hourly.to_excel(\"df_weather_hourly.xlsx\", index=False)\n",
        "\n",
        "print(f\" Saved hourly weather data ({len(df_weather_hourly)} records) to df_weather_hourly.xlsx\")\n"
      ],
      "metadata": {
        "id": "YBsnv0fJtrr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging the temperature data with fuel dataset"
      ],
      "metadata": {
        "id": "EiJ9x5l8ua6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to datetime index\n",
        "df_weather_hourly['datetime'] = pd.to_datetime(df_weather_hourly['datetime'])\n",
        "df_clean['Date_Hour_Desc'] = pd.to_datetime(df_clean['Date_Hour_Desc'])\n",
        "\n",
        "# Merge on DEVICE_ID + timestamp\n",
        "df_merged = pd.merge(\n",
        "    df_clean,\n",
        "    df_weather_hourly,\n",
        "    left_on=['DEVICE_ID', 'Date_Hour_Desc'],\n",
        "    right_on=['DEVICE_ID', 'datetime'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Drop redundant column\n",
        "df_merged = df_merged.drop(columns=['datetime'])\n",
        "print(f\" Merged dataset shape: {df_merged.shape}\")\n",
        "print(df_merged[['DEVICE_ID', 'Date_Hour_Desc', 'LevelLiters', 'temperature']].head())\n"
      ],
      "metadata": {
        "id": "SMLGkn3FuV8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Missing temperature count:\", df_merged['temperature'].isna().sum())\n",
        "print(\"NaN count in df_merged before cleaning:\", df_merged['LevelLiters'].isna().sum())"
      ],
      "metadata": {
        "id": "quNiPg1yupLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detecting refilling and noise spikes in raw fuel level data and classifying them"
      ],
      "metadata": {
        "id": "JXQtkTsGvKYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import zscore\n",
        "\n",
        "def detect_refills_with_datetime_index_raw(\n",
        "    df,\n",
        "    device_id,\n",
        "    z_threshold=3,\n",
        "    capacity_factor_refill=0.1,\n",
        "    capacity_factor_noise=0.02,\n",
        "    window_after=30,          # hours forward to test persistence\n",
        "    persist_points=8,\n",
        "    flat_std_threshold=1,     # liters tolerance for detecting flat plateaus\n",
        "    reverse_window_hours=30   # hours ahead to check for drop-back reversals\n",
        "):\n",
        "\n",
        "    # Filter one device\n",
        "    df_device = df[df[\"DEVICE_ID\"] == device_id].copy()\n",
        "\n",
        "    # Make sure time index is correct\n",
        "    if not np.issubdtype(df_device.index.dtype, np.datetime64):\n",
        "        if 'Date_Hour_Desc' in df_device.columns:\n",
        "            df_device['Date_Hour_Desc'] = pd.to_datetime(df_device['Date_Hour_Desc'], errors='coerce')\n",
        "            df_device = df_device.set_index('Date_Hour_Desc')\n",
        "        else:\n",
        "            raise ValueError(\" DataFrame must have a datetime index or 'Date_Hour_Desc' column.\")\n",
        "\n",
        "    df_device = df_device.sort_index()\n",
        "\n",
        "    # Compute diffs\n",
        "    df_device[\"diff\"] = df_device[\"LevelLiters\"].diff()\n",
        "\n",
        "    # Z-score spike candidates\n",
        "    df_device[\"zscore\"] = zscore(df_device[\"diff\"].fillna(0))\n",
        "    candidates = df_device.index[abs(df_device[\"zscore\"]) > z_threshold]\n",
        "\n",
        "    print(f\"Device {device_id}: Z-score spike candidates = {len(candidates)}\")\n",
        "\n",
        "    refills, noise = [], []\n",
        "\n",
        "    # A. Handle possible flat plateau at start\n",
        "    first_level = df_device.iloc[0]['LevelLiters']\n",
        "    first_capacity = df_device.iloc[0]['TotalLiters']\n",
        "\n",
        "    if first_level > 0.8 * first_capacity:\n",
        "        print(\" Initial high level detected at dataset start (possible calibration / noise).\")\n",
        "\n",
        "        start_flat_points = []\n",
        "        for t in df_device.index:\n",
        "            if abs(df_device.loc[t, \"LevelLiters\"] - first_level) <= flat_std_threshold:\n",
        "                start_flat_points.append(t)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        print(f\"Initial flat plateau length: {len(start_flat_points)} points\")\n",
        "        noise.extend(start_flat_points)\n",
        "\n",
        "    # B. Normal spike-based logic\n",
        "    for idx in candidates:\n",
        "        pos = df_device.index.get_loc(idx)\n",
        "        if pos == 0:\n",
        "            continue\n",
        "\n",
        "        jump = df_device.loc[idx, \"diff\"]\n",
        "        tank_capacity = df_device.loc[idx, \"TotalLiters\"]\n",
        "\n",
        "        # Thresholds\n",
        "        refill_jump_threshold = capacity_factor_refill * tank_capacity\n",
        "        noise_jump_threshold  = capacity_factor_noise * tank_capacity\n",
        "\n",
        "        prev_level = df_device.iloc[pos - 1][\"LevelLiters\"]\n",
        "\n",
        "        # Persistence check\n",
        "        window_end = idx + pd.Timedelta(hours=window_after)\n",
        "        future = df_device.loc[idx:window_end, \"LevelLiters\"]\n",
        "\n",
        "        persistent_threshold = max(0.01 * tank_capacity, 0.1 * jump)\n",
        "        persistent = (future > prev_level + persistent_threshold).sum() >= persist_points\n",
        "\n",
        "        # Detect drop-back within persistence window\n",
        "        early_reversal = (future < prev_level + 0.1 * tank_capacity).any()\n",
        "\n",
        "        # Reverse window (after persistence)\n",
        "        reversal_window = df_device.loc[\n",
        "            window_end : window_end + pd.Timedelta(hours=reverse_window_hours),\n",
        "            \"LevelLiters\"\n",
        "        ]\n",
        "        reversed_drop = (\n",
        "            (reversal_window < prev_level + 0.1 * tank_capacity).any()\n",
        "            if len(reversal_window) > 0\n",
        "            else False\n",
        "        )\n",
        "\n",
        "        # Detect flat plateau\n",
        "        plateau_level = df_device.loc[idx, \"LevelLiters\"]\n",
        "        after_idx = df_device.index[pos:]\n",
        "        flat_points = []\n",
        "        for t in after_idx:\n",
        "            if abs(df_device.loc[t, \"LevelLiters\"] - plateau_level) <= flat_std_threshold:\n",
        "                flat_points.append(t)\n",
        "            else:\n",
        "                break\n",
        "        flat_plateau = len(flat_points) > persist_points\n",
        "\n",
        "        # Decision\n",
        "        if jump > refill_jump_threshold and persistent and not reversed_drop and not early_reversal and not flat_plateau:\n",
        "            refills.append(idx)\n",
        "        else:\n",
        "            noise.extend(flat_points)\n",
        "\n",
        "    # Deduplicate and sort\n",
        "    noise = sorted(pd.Index(noise).unique())\n",
        "\n",
        "    print(f\"Device {device_id}: REFILLS = {len(refills)}, NOISE (incl. flats) = {len(noise)}\")\n",
        "\n",
        "    refill_df = df_device.loc[refills]\n",
        "    noise_df  = df_device.loc[noise]\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(14,6))\n",
        "    plt.plot(df_device.index, df_device[\"LevelLiters\"], color=\"steelblue\", label=\"Raw LevelLiters\", linewidth=2)\n",
        "\n",
        "    if len(refill_df) > 0:\n",
        "        plt.scatter(refill_df.index, refill_df[\"LevelLiters\"],\n",
        "                    color=\"green\", marker=\"^\", s=80, label=\"Refill Event\")\n",
        "    if len(noise_df) > 0:\n",
        "        plt.scatter(noise_df.index, df_device.loc[noise_df.index, \"LevelLiters\"],\n",
        "                    color=\"red\", marker=\"x\", s=70, label=\"Noise Spike\")\n",
        "\n",
        "    plt.title(f\"Refill & Noise Detection – Device {device_id}\", fontsize=14)\n",
        "    plt.xlabel(\"Timestamp\")\n",
        "    plt.ylabel(\"LevelLiters (raw)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return refill_df, noise_df, df_device, device_id\n",
        "refills, noise, processed, device_id = detect_refills_with_datetime_index_raw(df_merged, device_id=558)"
      ],
      "metadata": {
        "id": "LJoElizIuzB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning noisy spikes from LevelLiters (raw data), interpolating missing values, and resampling to hourly frequency."
      ],
      "metadata": {
        "id": "saM1s8AcwPqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_noise_interpolate(df_device, noise_df, device_id):\n",
        "\n",
        "    # Copy input device data\n",
        "    df_stl = df_device.copy()\n",
        "\n",
        "    # Replace noise points with NaN\n",
        "    df_stl.loc[noise_df.index, 'LevelLiters'] = np.nan\n",
        "\n",
        "    # Interpolate missing values (time-based)\n",
        "    df_stl['LevelLiters'] = df_stl['LevelLiters'].interpolate(method='time')\n",
        "\n",
        "    print(\"NaN count before resampling:\", df_stl['LevelLiters'].isna().sum())\n",
        "    print(\"Number of timestamps before resampling:\", len(df_stl))\n",
        "\n",
        "\n",
        "    # Ensure regular hourly frequency\n",
        "    df_stl = df_stl.asfreq('h')  # enforce 1-hour interval\n",
        "\n",
        "    df_stl['LevelLiters'] = (\n",
        "    df_stl['LevelLiters']\n",
        "    .interpolate(method='time', limit_direction='both')\n",
        "    .bfill()  # fill start\n",
        "    .ffill()  # fill end\n",
        "    )\n",
        "\n",
        "    print(\"NaN count after resampling:\", df_stl['LevelLiters'].isna().sum())\n",
        "    print(\"Number of timestamps after resampling:\", len(df_stl))\n",
        "\n",
        "\n",
        "    # Visual check\n",
        "    plt.figure(figsize=(14,6))\n",
        "    plt.plot(df_device.index, df_device['LevelLiters'],\n",
        "             color='lightgray', label='Original (with noise)', linewidth=1.5)\n",
        "    plt.plot(df_stl.index, df_stl['LevelLiters'],\n",
        "             color='steelblue', label='Cleaned (interpolated)', linewidth=2)\n",
        "    if len(noise_df) > 0:\n",
        "        plt.scatter(noise_df.index, df_device.loc[noise_df.index, 'LevelLiters'],\n",
        "                    color='red', marker='x', s=60, label='Removed Noise')\n",
        "    plt.legend()\n",
        "    plt.title(f\"Noise Removal and Interpolation – Device {device_id}\")\n",
        "    plt.xlabel(\"Timestamp\")\n",
        "    plt.ylabel(\"LevelLiters (Raw)\")\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Return cleaned data\n",
        "    return df_stl\n",
        "\n",
        "# Clean noise using detected indices\n",
        "df_stl = clean_noise_interpolate(processed, noise, device_id)\n",
        "\n",
        "# Check if NaNs remain\n",
        "print(\"Remaining NaN values per column:\\n\", df_stl.isna().sum())\n"
      ],
      "metadata": {
        "id": "79Lc-FuswMbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stl['diff'] = df_stl['diff'].fillna(0)\n",
        "nan_diff_rows = df_stl[df_stl['diff'].isna()]\n",
        "print(nan_diff_rows[['LevelLiters']].head(5))\n"
      ],
      "metadata": {
        "id": "adMXEtL3xR8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying temperature correction"
      ],
      "metadata": {
        "id": "2jZERw38w8yD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "BETA = 0.00089   # thermal expansion coefficient of diesel (1/°C)\n",
        "T_REF = 15       # reference temperature (°C) — baseline at which tank was calibrated\n",
        "\n",
        "# Compute temperature-corrected level\n",
        "df_stl['LevelLiters_corrected'] = df_stl['LevelLiters'] / (1 + BETA * (df_stl['temperature'] - T_REF))\n",
        "\n",
        "# Visualize the correction effect\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.plot(df_stl.index, df_stl['LevelLiters'], color='blue', alpha=0.6, label='Raw LevelLiters')\n",
        "plt.plot(df_stl.index, df_stl['LevelLiters_corrected'], color='darkorange', label='Temperature-corrected Level')\n",
        "plt.title(\"Temperature Correction Applied to Fuel Level\")\n",
        "plt.xlabel(\"Timestamp\")\n",
        "plt.ylabel(\"Fuel Level (Liters)\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Quick sanity check ---\n",
        "print(\"Temperature correction applied successfully!\")\n",
        "print(\"Raw vs Corrected correlation with temperature:\")\n",
        "print(\"Raw:      \", df_stl[['LevelLiters','temperature']].corr().iloc[0,1])\n",
        "print(\"Corrected:\", df_stl[['LevelLiters_corrected','temperature']].corr().iloc[0,1])"
      ],
      "metadata": {
        "id": "OYTmGjJDw7_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stl['temperature'] = df_stl['temperature'].interpolate(method='time')\n",
        "df_stl['LevelLiters_corrected'] = df_stl['LevelLiters_corrected'].interpolate(method='time')\n",
        "print(\"Any NaNs:\", df_stl.isna().any().any())\n",
        "print(\"Any duplicates:\", df_stl.index.duplicated().sum())\n"
      ],
      "metadata": {
        "id": "pClsIcvwx81i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detecting and cleaning outliers (IQR-based)"
      ],
      "metadata": {
        "id": "XV7X8cF4yNEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Compute temperature-corrected consumption ---\n",
        "df_stl['Consumption_corrected'] = -df_stl['LevelLiters_corrected'].diff()\n",
        "df_stl.loc[df_stl['Consumption_corrected'] < 0, 'Consumption_corrected'] = 0\n",
        "\n",
        "# Smoothing\n",
        "df_stl['Consumption_corrected'] = (\n",
        "    df_stl['Consumption_corrected']\n",
        "    .rolling(window=3, center=True, min_periods=1)\n",
        "    .mean()\n",
        ")\n",
        "\n",
        "# Detect and clean outliers (IQR-based)\n",
        "Q1, Q3 = df_stl[\"Consumption_corrected\"].quantile([0.25, 0.75])\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "outlier_mask = (\n",
        "    (df_stl[\"Consumption_corrected\"] < Q1 - 1.5 * IQR) |\n",
        "    (df_stl[\"Consumption_corrected\"] > Q3 + 1.5 * IQR)\n",
        ")\n",
        "\n",
        "print(f\"Detected {outlier_mask.sum()} outlier points — replacing with interpolated values\")\n",
        "\n",
        "# Replace outliers with NaN\n",
        "df_stl.loc[outlier_mask, \"Consumption_corrected\"] = pd.NA\n",
        "\n",
        "# Interpolate missing values (fill cleaned outliers smoothly)\n",
        "df_stl[\"Consumption_corrected\"] = df_stl[\"Consumption_corrected\"].interpolate(\n",
        "    method=\"linear\", limit_direction=\"both\"\n",
        ")\n",
        "\n",
        "# Final sanity check\n",
        "nan_count = df_stl[\"Consumption_corrected\"].isna().sum()\n",
        "print(f\"✅ Cleaning complete. Remaining NaN in 'Consumption': {nan_count}\")\n"
      ],
      "metadata": {
        "id": "f5tG0-FwyMKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forcasting Models"
      ],
      "metadata": {
        "id": "ZUWBBAzQ1mPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ETS Forecasting (Hourly Fuel Consumption for the last 30 days)"
      ],
      "metadata": {
        "id": "-G0Bzeb72hZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Select the hourly consumption series\n",
        "series = df_stl[\"Consumption_corrected\"].copy()\n",
        "series = series + 1e-6  # small offset to avoid zeros\n",
        "\n",
        "# Split into training and last 30 days (720 hours)\n",
        "train = series[:-720]\n",
        "test = series[-720:]\n",
        "\n",
        "# Fit ETS model\n",
        "ets_model = ExponentialSmoothing(\n",
        "    train,\n",
        "    trend=\"add\",\n",
        "    seasonal=\"add\",\n",
        "    seasonal_periods=24,   # 24-hour daily cycle for hourly data\n",
        "    initialization_method=\"estimated\"\n",
        ").fit()\n",
        "\n",
        "# Forecast the next 720 hours (30 days)\n",
        "forecast_horizon = 720\n",
        "forecast = ets_model.forecast(forecast_horizon)\n",
        "\n",
        "# Evaluate\n",
        "mae = mean_absolute_error(test, forecast)\n",
        "rmse = np.sqrt(mean_squared_error(test, forecast))\n",
        "nrmse = rmse / (np.max(test) - np.min(test))\n",
        "mase = mae / np.mean(np.abs(np.diff(train)))\n",
        "r2 = r2_score(test, forecast)\n",
        "\n",
        "\n",
        "\n",
        "print(\"ETS Forecast Evaluation – Last 30 Days\")\n",
        "print(\"-------------------------------------------\")\n",
        "print(f\"MAE  : {mae:.3f}\")\n",
        "print(f\"RMSE : {rmse:.3f}\")\n",
        "print(f\"NRMSE (range) : {nrmse*100:.2f}%\")\n",
        "print(f\"R²   : {r2:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(test.index, test, label=\"Actual (Last 30 Days)\", color=\"black\", linewidth=1.8)\n",
        "plt.plot(forecast.index, forecast, label=\"Predicted (ETS)\", color=\"blue\", linewidth=2)\n",
        "\n",
        "plt.title(f\"ETS Forecast (Hourly Fuel Consumption)-Device {int(device_id)}\", fontsize=14)\n",
        "plt.xlabel(\"Timestamp\")\n",
        "plt.ylabel(\"Fuel Consumption (Liters/hour)\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "plt.gcf().autofmt_xdate(rotation=45)   # auto-rotate to avoid overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hLIelY-B2Xpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LightGBM Forecasting"
      ],
      "metadata": {
        "id": "slGNm2ZG3Vng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare base hourly data\n",
        "df_hourly = df_stl[['Consumption_corrected', 'temperature']].copy()\n",
        "\n",
        "# Small offset to avoid division errors\n",
        "df_hourly['Consumption_corrected'] = df_hourly['Consumption_corrected'] + 1e-6\n",
        "\n",
        "df_hourly['y'] = df_hourly['Consumption_corrected']\n",
        "\n",
        "# Feature engineering\n",
        "# Lag features (past 24 hours, one day back)\n",
        "for lag in range(1, 25):\n",
        "    df_hourly[f'lag_{lag}'] = df_hourly['y'].shift(lag)\n",
        "\n",
        "# Rolling stats over past 24 hours\n",
        "df_hourly['rolling_mean_24'] = df_hourly['y'].shift(1).rolling(24).mean()\n",
        "df_hourly['rolling_std_24']  = df_hourly['y'].shift(1).rolling(24).std()\n",
        "\n",
        "# Temperature lag features\n",
        "df_hourly['temp_lag1'] = df_hourly['temperature'].shift(1)\n",
        "df_hourly['temp_mean_24'] = df_hourly['temperature'].shift(1).rolling(24).mean()\n",
        "df_hourly['temp_std_24']  = df_hourly['temperature'].shift(1).rolling(24).std()\n",
        "\n",
        "# Time-based features\n",
        "df_hourly['hour'] = df_hourly.index.hour\n",
        "df_hourly['dayofweek'] = df_hourly.index.dayofweek\n",
        "df_hourly['is_weekend'] = (df_hourly['dayofweek'] >= 5).astype(int)\n",
        "\n",
        "# Drop missing values created by shifting/rolling\n",
        "df_hourly = df_hourly.dropna()\n",
        "\n",
        "\n",
        "# Train/Test split (last 30 days = 720 hours)\n",
        "train = df_hourly.iloc[:-720]\n",
        "test  = df_hourly.iloc[-720:]\n",
        "\n",
        "X_train = train.drop(columns=['y', 'Consumption_corrected'])\n",
        "y_train = train['y']\n",
        "X_test = test.drop(columns=['y', 'Consumption_corrected'])\n",
        "y_test = test['y']\n",
        "\n",
        "\n",
        "# Train LightGBM model\n",
        "lgb_model = lgb.LGBMRegressor(\n",
        "    n_estimators=600,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=10,\n",
        "    num_leaves=31,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "lgb_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Forecast and evaluate\n",
        "y_pred = lgb_model.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "nrmse = rmse / (np.max(y_test) - np.min(y_test))\n",
        "mase = mae / np.mean(np.abs(np.diff(y_train)))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(\" LightGBM Forecast Evaluation – Last 30 Days (Hourly)\")\n",
        "print(\"-------------------------------------------\")\n",
        "print(f\"MAE  : {mae:.3f}\")\n",
        "print(f\"RMSE : {rmse:.3f}\")\n",
        "print(f\"NRMSE (range) : {nrmse*100:.2f}%\")\n",
        "print(f\"MASE : {mase:.2f}\")\n",
        "print(f\"R²   : {r2:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(y_test.index, y_test, label=\"Actual (Last 30 Days)\", color=\"black\", linewidth=1.6)\n",
        "plt.plot(y_test.index, y_pred, label=\"Predicted (LightGBM)\", color=\"orange\", linewidth=1.8)\n",
        "plt.title(f\"LightGBM Forecast (Hourly Fuel Consumption)-Device {int(device_id)}\")\n",
        "plt.xlabel(\"Timestamp\")\n",
        "plt.ylabel(\"Fuel Consumption (Liters/hour)\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "\n",
        "# Date tick formatting\n",
        "plt.gcf().autofmt_xdate(rotation=45)   # auto-rotate to avoid overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Feature importance\n",
        "# ---------------------------------------------------------------\n",
        "feat_imp = pd.Series(lgb_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=feat_imp[:10], y=feat_imp.index[:10], color='orange')\n",
        "plt.title(f\"Top 10 Feature Importances – LightGBM (Device {int(device_id)})\")\n",
        "plt.xlabel(\"Relative Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Hkmv14iR3Ubj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost Forecast"
      ],
      "metadata": {
        "id": "yXf5c9CT4e8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare hourly dataset\n",
        "df_hourly = df_stl[['Consumption_corrected', 'temperature']].copy()\n",
        "\n",
        "# Target variable\n",
        "df_hourly['y'] = df_hourly['Consumption_corrected']\n",
        "\n",
        "\n",
        "# Feature engineering\n",
        "# Lag features (past 24 hours)\n",
        "for lag in range(1, 25):\n",
        "    df_hourly[f'lag_{lag}'] = df_hourly['y'].shift(lag)\n",
        "\n",
        "# Rolling statistics (past 24 hours window)\n",
        "df_hourly['rolling_mean_24'] = df_hourly['y'].shift(1).rolling(24).mean()\n",
        "df_hourly['rolling_std_24']  = df_hourly['y'].shift(1).rolling(24).std()\n",
        "\n",
        "# Temperature-based features\n",
        "df_hourly['temp_lag1']  = df_hourly['temperature'].shift(1)\n",
        "df_hourly['temp_mean24'] = df_hourly['temperature'].shift(1).rolling(24).mean()\n",
        "df_hourly['temp_std24']  = df_hourly['temperature'].shift(1).rolling(24).std()\n",
        "\n",
        "# Calendar/time-based features\n",
        "df_hourly['hour'] = df_hourly.index.hour\n",
        "df_hourly['dayofweek'] = df_hourly.index.dayofweek\n",
        "df_hourly['is_weekend'] = (df_hourly['dayofweek'] >= 5).astype(int)\n",
        "\n",
        "# Drop rows with NaNs from shifting/rolling\n",
        "df_hourly = df_hourly.dropna()\n",
        "\n",
        "\n",
        "# Train/test split (last 30 days = 720 hours)\n",
        "train = df_hourly.iloc[:-720]\n",
        "test  = df_hourly.iloc[-720:]\n",
        "\n",
        "X_train = train.drop(columns=['y', 'Consumption_corrected'])\n",
        "y_train = train['y']\n",
        "X_test  = test.drop(columns=['y', 'Consumption_corrected'])\n",
        "y_test  = test['y']\n",
        "\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=600,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=8,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    reg_lambda=1.0\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Forecast and evaluate\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "nrmse = rmse / (np.max(y_test) - np.min(y_test))\n",
        "mase = mae / np.mean(np.abs(np.diff(y_train)))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(\" LightGBM Forecast Evaluation – Last 30 Days (Hourly)\")\n",
        "print(\"-------------------------------------------\")\n",
        "print(f\"MAE  : {mae:.3f}\")\n",
        "print(f\"RMSE : {rmse:.3f}\")\n",
        "print(f\"NRMSE (range) : {nrmse*100:.2f}%\")\n",
        "print(f\"MASE : {mase:.2f}\")\n",
        "print(f\"R²   : {r2:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Plot actual vs predicted (Zoomed on last 30 days)\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(y_test.index, y_test, label=\"Actual (Last 30 Days)\", color=\"black\", linewidth=1.6)\n",
        "plt.plot(y_test.index, y_pred, label=\"Predicted (XGBoost)\", color=\"red\", linewidth=2)\n",
        "plt.title(f\"XGBoost Forecast (Hourly Fuel Consumption)-Device {int(device_id)}\", fontsize=13)\n",
        "plt.xlabel(\"Timestamp\")\n",
        "plt.ylabel(\"Fuel Consumption (Liters/hour)\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "plt.gcf().autofmt_xdate(rotation=45)   # auto-rotate to avoid overlap\n",
        "plt.tight_layout()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Feature importance\n",
        "# ---------------------------------------------------------------\n",
        "feat_imp = pd.Series(xgb_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=feat_imp[:10], y=feat_imp.index[:10], color='crimson')\n",
        "plt.title(f\"Top 10 Feature Importances – XGBoost (Device {int(device_id)})\", fontsize=12)\n",
        "plt.xlabel(\"Relative Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VRrJEhvf4dOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Forecast"
      ],
      "metadata": {
        "id": "d-pJlfH25gvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "# Prepare hourly dataset\n",
        "df_hourly = df_stl[['Consumption_corrected', 'temperature']].copy().dropna()\n",
        "df_hourly['y'] = df_hourly['Consumption_corrected']\n",
        "\n",
        "# Feature engineering\n",
        "# Lag features (past 24 hours)\n",
        "for lag in range(1, 25):\n",
        "    df_hourly[f'lag_{lag}'] = df_hourly['y'].shift(lag)\n",
        "\n",
        "# Rolling window features\n",
        "df_hourly['rolling_mean_24'] = df_hourly['y'].shift(1).rolling(24).mean()\n",
        "df_hourly['rolling_std_24']  = df_hourly['y'].shift(1).rolling(24).std()\n",
        "\n",
        "# Temperature features\n",
        "df_hourly['temp_lag1']   = df_hourly['temperature'].shift(1)\n",
        "df_hourly['temp_mean24'] = df_hourly['temperature'].shift(1).rolling(24).mean()\n",
        "df_hourly['temp_std24']  = df_hourly['temperature'].shift(1).rolling(24).std()\n",
        "\n",
        "# Calendar/time features\n",
        "df_hourly['hour']       = df_hourly.index.hour\n",
        "df_hourly['dayofweek']  = df_hourly.index.dayofweek\n",
        "df_hourly['is_weekend'] = (df_hourly['dayofweek'] >= 5).astype(int)\n",
        "\n",
        "# Drop NaN rows created by lags/rolling windows\n",
        "df_hourly = df_hourly.dropna()\n",
        "\n",
        "# Train/Test split (last 30 days = 720 hours)\n",
        "train = df_hourly.iloc[:-720]\n",
        "test  = df_hourly.iloc[-720:]\n",
        "\n",
        "X_train = train.drop(columns=['y', 'Consumption_corrected'])\n",
        "y_train = train['y']\n",
        "X_test  = test.drop(columns=['y', 'Consumption_corrected'])\n",
        "y_test  = test['y']\n",
        "\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=400,\n",
        "    random_state=42,\n",
        "    max_depth=15,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Forecast and evaluate\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "mae  = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "nrmse = rmse / (np.max(y_test) - np.min(y_test))\n",
        "mase = mae / np.mean(np.abs(np.diff(y_train)))\n",
        "r2   = r2_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(\" Random Forest Forecast Evaluation – Last 30 Days (Hourly)\")\n",
        "print(\"-------------------------------------------\")\n",
        "print(f\"MAE  : {mae:.4f}\")\n",
        "print(f\"RMSE : {rmse:.4f}\")\n",
        "print(f\"NRMSE (range): {nrmse*100:.2f}%\")\n",
        "print(f\"MASE : {mase:.2f}\")\n",
        "print(f\"R²   : {r2:.3f}\")\n",
        "\n",
        "\n",
        "# Plot actual vs predicted (Full 30 Days)\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(y_test.index, y_test, color='black', linewidth=1.5, label='Actual (Last 30 Days)')\n",
        "plt.plot(y_test.index, y_pred, color='purple', linewidth=2, label='Predicted (Random Forest)')\n",
        "plt.title(f\"Random Forest Forecast (Hourly Fuel Consumption)-Device {int(device_id)}\", fontsize=13)\n",
        "plt.xlabel(\"Timestamp\")\n",
        "plt.ylabel(\"Fuel Consumption (Liters/hour)\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "plt.gcf().autofmt_xdate(rotation=45)   # auto-rotate to avoid overlap\n",
        "plt.tight_layout()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Feature importance\n",
        "# ---------------------------------------------------------------\n",
        "feat_imp = pd.Series(rf_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=feat_imp[:10], y=feat_imp.index[:10], color='purple')\n",
        "plt.title(f\"Top 10 Feature Importances – Random Forest (Device {int(device_id)})\")\n",
        "plt.xlabel(\"Relative Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sccsD4wU5iUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM Forecast"
      ],
      "metadata": {
        "id": "9CXzxCB56kp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Prepare hourly dataset\n",
        "df_hourly = df_stl[['Consumption_corrected', 'temperature']].copy().dropna()\n",
        "df_hourly['y'] = df_hourly['Consumption_corrected']\n",
        "\n",
        "\n",
        "# Feature engineering\n",
        "# Lag features (past 24 hours)\n",
        "for lag in range(1, 25):\n",
        "    df_hourly[f'lag_{lag}'] = df_hourly['y'].shift(lag)\n",
        "\n",
        "# Rolling window stats (24-hour window)\n",
        "df_hourly['rolling_mean_24'] = df_hourly['y'].shift(1).rolling(24).mean()\n",
        "df_hourly['rolling_std_24']  = df_hourly['y'].shift(1).rolling(24).std()\n",
        "\n",
        "# Temperature features\n",
        "df_hourly['temp_lag1']   = df_hourly['temperature'].shift(1)\n",
        "df_hourly['temp_mean24'] = df_hourly['temperature'].shift(1).rolling(24).mean()\n",
        "df_hourly['temp_std24']  = df_hourly['temperature'].shift(1).rolling(24).std()\n",
        "\n",
        "# Calendar features\n",
        "df_hourly['hour']       = df_hourly.index.hour\n",
        "df_hourly['dayofweek']  = df_hourly.index.dayofweek\n",
        "df_hourly['is_weekend'] = (df_hourly['dayofweek'] >= 5).astype(int)\n",
        "\n",
        "# Drop NaNs from lag/rolling feature creation\n",
        "df_hourly = df_hourly.dropna()\n",
        "\n",
        "\n",
        "# Train/Test split (last 30 days = 720 hours)\n",
        "train = df_hourly.iloc[:-720]\n",
        "test  = df_hourly.iloc[-720:]\n",
        "\n",
        "X_train = train.drop(columns=['y', 'Consumption_corrected'])\n",
        "y_train = train['y']\n",
        "X_test  = test.drop(columns=['y', 'Consumption_corrected'])\n",
        "y_test  = test['y']\n",
        "\n",
        "# Scaling\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled  = scaler_X.transform(X_test)\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_scaled  = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "# Reshape for LSTM: [samples, timesteps, features]\n",
        "X_train_3d = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
        "X_test_3d  = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
        "\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train_3d.shape[1], X_train_3d.shape[2]), return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Early stopping callback\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_3d, y_train_scaled,\n",
        "    validation_data=(X_test_3d, y_test_scaled),\n",
        "    epochs=200,\n",
        "    batch_size=8,\n",
        "    verbose=1,\n",
        "    shuffle=False,\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "\n",
        "\n",
        "# Forecast and evaluate\n",
        "y_pred_scaled = model.predict(X_test_3d)\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled).flatten()\n",
        "y_true = y_test.values.flatten()\n",
        "\n",
        "mae  = mean_absolute_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "nrmse = rmse / (np.max(y_true) - np.min(y_true))\n",
        "mase = mae / np.mean(np.abs(np.diff(y_train)))\n",
        "r2   = r2_score(y_true, y_pred)\n",
        "\n",
        "print(\" LSTM Forecast Evaluation – Last 30 Days (Hourly)\")\n",
        "print(\"-------------------------------------------\")\n",
        "print(f\"MAE  : {mae:.4f}\")\n",
        "print(f\"RMSE : {rmse:.4f}\")\n",
        "print(f\"NRMSE (range): {nrmse*100:.2f}%\")\n",
        "print(f\"MASE : {mase:.2f}\")\n",
        "print(f\"R²   : {r2:.3f}\")\n",
        "\n",
        "\n",
        "# Training Loss Visualization\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(history.history['loss'], label='Train Loss', color='purple')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
        "plt.title(\"LSTM Training & Validation Loss (Hourly, Early Stopping)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Actual vs Predicted (Full 30 Days)\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(y_test.index, y_true, label=\"Actual (Last 30 Days)\", color=\"black\", linewidth=1.5)\n",
        "plt.plot(y_test.index, y_pred, label=\"Predicted (LSTM)\", color=\"brown\", linewidth=2)\n",
        "plt.title(f\"LSTM Forecast (Hourly Fuel Consumption)-Device {int(device_id)}\", fontsize=13)\n",
        "plt.xlabel(\"Timestamp\")\n",
        "plt.ylabel(\"Fuel Consumption (Liters/hour)\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "plt.gcf().autofmt_xdate(rotation=45)   # auto-rotate to avoid overlap\n",
        "plt.tight_layout()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uSKJHnZq6XDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STL Decomposition"
      ],
      "metadata": {
        "id": "vi3P4feT7SC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STL Decomposition – Daily Temp-Corrected Fuel Consumption"
      ],
      "metadata": {
        "id": "R2lcqR607dOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.seasonal import STL\n",
        "\n",
        "\n",
        "# Prepare the daily dataset\n",
        "# Aggregate hourly data to daily mean (or sum if you want total consumption)\n",
        "df_daily_stl = df_stl[['Consumption_corrected']].resample('D').sum()\n",
        "df_daily_stl = df_daily_stl.dropna()\n",
        "\n",
        "\n",
        "# Perform STL decomposition\n",
        "stl = STL(df_daily_stl['Consumption_corrected'], period=7, robust=True)\n",
        "result = stl.fit()\n",
        "\n",
        "\n",
        "# Visualize decomposition\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.suptitle(\"STL Decomposition – Daily Temperature-Corrected Fuel Consumption\", fontsize=14, y=0.94)\n",
        "\n",
        "plt.subplot(4, 1, 1)\n",
        "plt.plot(df_daily_stl.index, df_daily_stl['Consumption_corrected'], color='black')\n",
        "plt.title(\"Observed\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(4, 1, 2)\n",
        "plt.plot(df_daily_stl.index, result.trend, color='blue')\n",
        "plt.title(\"Trend\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(4, 1, 3)\n",
        "plt.plot(df_daily_stl.index, result.seasonal, color='orange')\n",
        "plt.title(\"Seasonal (Weekly Pattern)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(4, 1, 4)\n",
        "plt.plot(df_daily_stl.index, result.resid, color='gray')\n",
        "plt.title(\"Residual (Noise)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Combine components into a single DataFrame\n",
        "df_stl_decomp = pd.DataFrame({\n",
        "    'Observed': df_daily_stl['Consumption_corrected'],\n",
        "    'Trend': result.trend,\n",
        "    'Seasonal': result.seasonal,\n",
        "    'Residual': result.resid\n",
        "})\n",
        "\n",
        "print(\"STL decomposition complete.\")\n",
        "print(df_stl_decomp.head())"
      ],
      "metadata": {
        "id": "UWJASz8c7d1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monthly Trend of Fuel Consumption (Based on STL Trend)"
      ],
      "metadata": {
        "id": "fXdbOTGT8DuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate to monthly totals using STL components\n",
        "df_monthly_stl = (\n",
        "    df_daily_result[\"Trend\"]\n",
        "    .resample(\"M\")\n",
        "    .sum()\n",
        "    .to_frame(\"Monthly_Consumption\")\n",
        ")\n",
        "\n",
        "df_monthly_stl[\"Year\"] = df_monthly_stl.index.year\n",
        "df_monthly_stl[\"MonthNum\"] = df_monthly_stl.index.month\n",
        "\n",
        "# Pivot: Month (1–12) vs Year\n",
        "pivot = df_monthly_stl.pivot_table(\n",
        "    index=\"MonthNum\",\n",
        "    columns=\"Year\",\n",
        "    values=\"Monthly_Consumption\",\n",
        "    aggfunc=\"sum\"\n",
        ").reindex(range(1, 13))  # ensure Jan–Dec order\n",
        "\n",
        "# Plot with smoothing (NO extrapolation)\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "for year in pivot.columns:\n",
        "    y = pivot[year]\n",
        "\n",
        "    # keep only months with real data\n",
        "    mask = y.notna()\n",
        "    x_real = y.index[mask]\n",
        "    y_real = y[mask]\n",
        "\n",
        "    # 3-month centered rolling mean\n",
        "    y_smooth = y_real.rolling(\n",
        "        window=3,\n",
        "        center=True,\n",
        "        min_periods=1\n",
        "    ).mean()\n",
        "\n",
        "    plt.plot(\n",
        "        x_real,\n",
        "        y_smooth,\n",
        "        linewidth=2.2,\n",
        "        label=str(year)\n",
        "    )\n",
        "\n",
        "# Formatting\n",
        "\n",
        "plt.title(f\"Monthly Trend of Fuel Consumption (Based on STL Trend)-Device {int(device_id)}\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Fuel Consumption (Liters)\")\n",
        "\n",
        "plt.xticks(\n",
        "    range(1, 13),\n",
        "    [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\n",
        "     \"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
        ")\n",
        "\n",
        "plt.grid(True, alpha=0.25)\n",
        "plt.legend(title=\"Year\", loc=\"upper right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wcaW7pRS79mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average Hourly Fuel Consumption (Daily Cycle)"
      ],
      "metadata": {
        "id": "nR4Pacye8jdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_stl[\"hour\"] = df_stl.index.hour\n",
        "df_stl.groupby(\"hour\")[\"Consumption\"].mean().plot(title=f\"Average Hourly Fuel Consumption (Daily Cycle)-Device {int(device_id)}\")"
      ],
      "metadata": {
        "id": "-ybkpnYG8ize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total Daily Fuel Use by Weekday"
      ],
      "metadata": {
        "id": "l5vsyx-l82OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create daily totals for total daily fuel use\n",
        "df_daily = df_stl[\"Consumption\"].resample(\"D\").sum()\n",
        "\n",
        "# Add weekday names to daily data\n",
        "df_daily = df_daily.to_frame(name=\"Daily_Total\")\n",
        "df_daily[\"weekday_name\"] = df_daily.index.day_name()\n",
        "\n",
        "# Define weekday order for proper x-axis order\n",
        "order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "\n",
        "# Plot both daily and weekly patterns\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Left: Daily (hourly pattern)\n",
        "df_stl.groupby(\"hour\")[\"Consumption\"].mean().plot(ax=axes[0], color=\"steelblue\")\n",
        "axes[0].set_title(f\"Average Hourly Fuel Consumption (Daily Cycle)-Device {int(device_id)}\")\n",
        "axes[0].set_xlabel(\"Hour of Day\")\n",
        "axes[0].set_ylabel(\"Liters per Hour\")\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Right: Weekly (total daily fuel use)\n",
        "df_daily.groupby(\"weekday_name\")[\"Daily_Total\"].mean().reindex(order).plot(\n",
        "    kind=\"bar\", ax=axes[1], color=\"teal\"\n",
        ")\n",
        "axes[1].set_title(f\"Total Daily Fuel Use by Weekday-Device {int(device_id)}\")\n",
        "axes[1].set_xlabel(\"Day of Week\")\n",
        "axes[1].set_ylabel(\"Total Fuel Used (Liters per Day)\")\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z_DXf8q48r58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average Hourly Fuel Consumption by Day of Week"
      ],
      "metadata": {
        "id": "4dtOuzh19JeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure datetime index\n",
        "df_stl = df_stl.copy()\n",
        "df_stl.index = pd.to_datetime(df_stl.index)\n",
        "\n",
        "# Extract hour and weekday\n",
        "df_stl[\"hour\"] = df_stl.index.hour\n",
        "df_stl[\"weekday\"] = df_stl.index.day_name()\n",
        "\n",
        "# Compute average consumption per (day, hour)\n",
        "pivot = (\n",
        "    df_stl.groupby([\"weekday\", \"hour\"])[\"Consumption\"]\n",
        "    .mean()\n",
        "    .unstack(level=1)\n",
        ")\n",
        "\n",
        "# Reorder days of the week (so Monday starts first)\n",
        "days_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "pivot = pivot.reindex(days_order)\n",
        "\n",
        "# PLOT\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(\n",
        "    pivot,\n",
        "    cmap=\"YlOrRd\",\n",
        "    linewidths=0.3,\n",
        "    linecolor=\"gray\",\n",
        "    cbar_kws={'label': 'Avg Fuel Consumption (L/h)'}\n",
        ")\n",
        "plt.title(f\"Average Hourly Fuel Consumption by Day of Week – Device {int(device_id)}\")\n",
        "plt.xlabel(\"Hour of Day\")\n",
        "plt.ylabel(\"Day of Week\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "p1Ee22_c89kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anomaly Detection extracted from residual component of STL"
      ],
      "metadata": {
        "id": "6OL6arJ1-Lq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare both residual signals ===\n",
        "resid_clean = df_stl_decomp[\"Residual\"]              # From STL after cleaning\n",
        "resid_raw = raw_resid.reindex_like(resid_clean)        # Align index with cleaned version\n",
        "\n",
        "# Compute anomaly thresholds (using ±3σ rule)\n",
        "mean_clean, std_clean = resid_clean.mean(), resid_clean.std()\n",
        "mean_raw, std_raw = resid_raw.mean(), resid_raw.std()\n",
        "\n",
        "upper_clean, lower_clean = mean_clean + 3 * std_clean, mean_clean - 3 * std_clean\n",
        "upper_raw, lower_raw = mean_raw + 3 * std_raw, mean_raw - 3 * std_raw\n",
        "\n",
        "# Flag anomalies\n",
        "anom_clean = resid_clean[(resid_clean > upper_clean) | (resid_clean < lower_clean)]\n",
        "anom_raw = resid_raw[(resid_raw > upper_raw) | (resid_raw < lower_raw)]\n",
        "\n",
        "print(f\"Detected anomalies (raw residuals): {len(anom_raw)}\")\n",
        "print(f\"Detected anomalies (cleaned residuals): {len(anom_clean)}\")\n",
        "\n",
        "# Plot comparison\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
        "\n",
        "# Raw residuals\n",
        "axes[0].plot(resid_raw.index, resid_raw, color=\"gray\", label=\"Raw Residuals\")\n",
        "axes[0].axhline(upper_raw, color=\"red\", linestyle=\"--\", label=\"+3σ Threshold\")\n",
        "axes[0].axhline(lower_raw, color=\"red\", linestyle=\"--\")\n",
        "axes[0].scatter(anom_raw.index, anom_raw, color=\"orange\", s=40, label=\"Anomalies\")\n",
        "axes[0].set_title(\"Anomaly Detection Using Raw Residuals (Before Cleaning)\")\n",
        "axes[0].set_ylabel(\"Residual (Liters/day)\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.4)\n",
        "\n",
        "# Cleaned residuals\n",
        "axes[1].plot(resid_clean.index, resid_clean, color=\"gray\", label=\"Cleaned Residuals\")\n",
        "axes[1].axhline(upper_clean, color=\"red\", linestyle=\"--\", label=\"+3σ Threshold\")\n",
        "axes[1].axhline(lower_clean, color=\"red\", linestyle=\"--\")\n",
        "axes[1].scatter(anom_clean.index, anom_clean, color=\"orange\", s=40, label=\"Anomalies\")\n",
        "axes[1].set_title(\"Anomaly Detection Using Cleaned STL Residuals (After IQR-Based Cleaning)\")\n",
        "axes[1].set_xlabel(\"Date\")\n",
        "axes[1].set_ylabel(\"Residual (Liters/day)\")\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.4)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VWx8J4eb9z5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seasonality analysis of fuel consumption"
      ],
      "metadata": {
        "id": "JG8ODRx-A9-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the style\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "# Prepare monthly summary\n",
        "df_monthly = df_daily_result.copy()\n",
        "df_monthly[\"Year\"] = df_monthly.index.year\n",
        "df_monthly[\"Month\"] = df_monthly.index.month\n",
        "\n",
        "# Compute total or mean fuel use per month/year\n",
        "monthly_summary = (\n",
        "    df_monthly.groupby([\"Year\", \"Month\"])[\"Seasonal\"]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Create abbreviated month names\n",
        "month_names = {1: \"Jan\", 2: \"Feb\", 3: \"Mar\", 4: \"Apr\", 5: \"May\", 6: \"Jun\",\n",
        "               7: \"Jul\", 8: \"Aug\", 9: \"Sep\", 10: \"Oct\", 11: \"Nov\", 12: \"Dec\"}\n",
        "monthly_summary[\"Month_Name\"] = monthly_summary[\"Month\"].map(month_names)\n",
        "\n",
        "# Order for faceting\n",
        "monthly_summary[\"Month_Name\"] = pd.Categorical(\n",
        "    monthly_summary[\"Month_Name\"],\n",
        "    categories=list(month_names.values()),\n",
        "    ordered=True\n",
        ")\n",
        "\n",
        "# Create facet grid (seasonality panel)\n",
        "fig = plt.figure(figsize=(16, 3.5))\n",
        "\n",
        "g = sns.FacetGrid(\n",
        "    monthly_summary,\n",
        "    col=\"Month_Name\",\n",
        "    col_wrap=12,\n",
        "    height=3,\n",
        "    aspect=0.6,\n",
        "    sharey=True,\n",
        "    gridspec_kws={\"hspace\": 0.05, \"wspace\": 0.05}\n",
        ")\n",
        "\n",
        "# Plot the line and mean line\n",
        "g.map_dataframe(sns.lineplot, x=\"Year\", y=\"Seasonal\", color=\"black\", linewidth=1.2)\n",
        "g.map_dataframe(lambda data, **kws: plt.axhline(\n",
        "    data[\"Seasonal\"].mean(),\n",
        "    color=\"blue\",\n",
        "    linewidth=2.5,\n",
        "    alpha=0.9\n",
        "))\n",
        "\n",
        "# Formatting\n",
        "g.set_titles(\"{col_name}\", fontsize=10, fontweight='bold', pad=8)\n",
        "\n",
        "# Customize each axis\n",
        "for i, ax in enumerate(g.axes.flat):\n",
        "    # Set gray background\n",
        "    ax.set_facecolor('#e8e8e8')\n",
        "\n",
        "    # White gridlines\n",
        "    ax.grid(True, axis='both', linestyle='-', linewidth=0.8, color='white', alpha=0.9)\n",
        "\n",
        "    # Rotate x-axis labels to vertical\n",
        "    ax.tick_params(axis='x', rotation=90, labelsize=7)\n",
        "    ax.tick_params(axis='y', labelsize=8)\n",
        "\n",
        "    # Format x-axis to show years properly\n",
        "    ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True, nbins=8))\n",
        "\n",
        "    # Remove spines\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_visible(False)\n",
        "\n",
        "    # Remove x-axis label\n",
        "    ax.set_xlabel(\"\")\n",
        "\n",
        "    # Hide y-axis labels for all except the first subplot\n",
        "    if i == 0:\n",
        "        ax.tick_params(axis='y', which='both', length=4, labelsize=8)\n",
        "        ax.set_ylabel(\"Fuel Consumption (Liters)\", fontsize=9)\n",
        "    else:\n",
        "        ax.tick_params(axis='y', which='both', left=False, labelleft=False)\n",
        "        ax.set_ylabel(\"\")\n",
        "\n",
        "# Adjust spacing to make room for bottom labels\n",
        "plt.subplots_adjust(left=0.05, right=0.98, top=0.92, bottom=0.18)\n",
        "\n",
        "# Add x-axis label at bottom\n",
        "g.fig.suptitle(\"month\",\n",
        "               fontsize=11, y=0.01, fontweight='normal')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aXl3ZDtIAkY0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}